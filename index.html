<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <img id="mouth-image" src="silent.png">
    <div id="debug-stats">
        <p>loudness: <span id="amplitude">0</span></p>
        <p>freq: <span id="frequency">0</span> Hz</p>
        <p>state: <span id="state">silent</span></p>
    </div>

    <script>
        const images = {
            silent: "silent.png",
            speaking: "speaking.png",
            a: "a.png",
            e: "e.png",
            i: "i.png",
            o: "o.png",
            u: "u.png" 
        };

        const vowelFrequencies = {
            'a': [600, 750],
            'e': [400, 2300],
            'i': [100, 300],
            'o': [400, 600],
            'u': [280, 380]
        };

        navigator.mediaDevices.getUserMedia({ audio: true })
            .then((stream) => {
                const audioContext = new AudioContext();
                const analyser = audioContext.createAnalyser();
                const microphone = audioContext.createMediaStreamSource(stream);
                microphone.connect(analyser);

                analyser.fftSize = 128;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);

                const detectVowel = () => {
                    analyser.getByteFrequencyData(dataArray);

                    const totalAmplitude = dataArray.reduce((sum, value) => sum + value, 0);

                    let maxAmplitude = 0;
                    let dominantFrequency = 0;
                    for (let i = 0; i < bufferLength; i++) {
                        if (dataArray[i] > maxAmplitude) {
                            maxAmplitude = dataArray[i];
                            dominantFrequency = i * (audioContext.sampleRate / analyser.fftSize);
                        }
                    }

                    let state = 'silent';
                    if (totalAmplitude >= 1000) {
                        state = 'speaking';
                        for (const [vowel, [low, high]] of Object.entries(vowelFrequencies)) {
                            if (dominantFrequency >= low && dominantFrequency <= high) {
                                state = vowel;
                                break;
                            }
                        }
                    }

                    return { state, totalAmplitude, dominantFrequency };
                };

                const mouthImage = document.getElementById('mouth-image');
                const amplitudeElement = document.getElementById('amplitude');
                const frequencyElement = document.getElementById('frequency');
                const stateElement = document.getElementById('state');

                const update = () => {
                    const { state, totalAmplitude, dominantFrequency } = detectVowel();
                    mouthImage.src = images[state] || images.silent;

                    amplitudeElement.textContent = totalAmplitude.toFixed(2);
                    frequencyElement.textContent = dominantFrequency.toFixed(2);
                    stateElement.textContent = state;

                    requestAnimationFrame(update);
                };

                update();
            })
    </script>
</body>
</html>